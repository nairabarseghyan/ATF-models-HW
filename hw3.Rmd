---
title: "HW3"
author: "Naira Maria Barseghyan"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
library(dplyr)
library(tidyr)
library(ggplot2)
library(ggpubr) 
library(knitr)
library(zoo)
library(survival)
library(simsurv) 
library(survminer)
library(pec)
library(SurvRegCensCov)
library(flexsurv) 
library(mstate)
library(igraph)
library(viridis)
library(RColorBrewer)
```

```{r, include=FALSE}
telco <- read.csv('telco.csv')
summary(telco)
head(telco)
```


```{r, include=FALSE}
# data preperation
telco$churn<-ifelse(telco$churn=='Yes',1,0)
telco$marital <- as.factor(telco$marital)
telco$ed <- as.factor(telco$ed)
telco$retire <- as.factor(telco$retire)
telco$gender <- as.factor(telco$gender)
telco$voice <- as.factor(telco$voice)
telco$internet <- as.factor(telco$internet)
telco$forward <- as.factor(telco$forward)
telco$custcat <- as.factor(telco$custcat)
```


```{r, include=FALSE}
surv_obj <- Surv(time=telco$tenure, event=telco$churn)
fit_aft_model <- function(dist) {
  model <- survreg(surv_obj ~ age + marital + address + income + ed + retire + gender + voice + internet + forward + custcat, data = telco, dist = dist)
  return(model)
}
```

```{r, include=FALSE}
distributions<-names(survreg.distributions)
```


```{r, include=FALSE}
models <- lapply(distributions, fit_aft_model)
```


```{r, include=FALSE}
new_data <- data.frame(
  age = mean(telco$age), 
  marital = as.factor(names(which.max(table(telco$marital)))),
  address = mean(telco$address), 
  income = mean(telco$income), 
  ed = as.factor(names(which.max(table(telco$ed)))),
  retire = as.factor(names(which.max(table(telco$retire)))),
  gender = as.factor(names(which.max(table(telco$gender)))),
  voice = as.factor(names(which.max(table(telco$voice)))),
  internet = as.factor(names(which.max(table(telco$internet)))),
  forward = as.factor(names(which.max(table(telco$forward)))),
  custcat = as.factor(names(which.max(table(telco$custcat)))),
  tenure = median(telco$tenure) # Median tenure value for prediction
)
```


```{r, include=FALSE}
survival_curves<- function(models,dist) {
      probs = seq(.1,.9,length=9)
      all_data <- data.frame()
  # Iterate through models and add to the ggplot object
  for (i in seq_along(models)) {
      probs = seq(.1,.9,length=9)
  
      # Predict survival probabilities using the fitted model
      pred_surv <- predict(models[[i]], type="quantile", p=1-probs, newdata = new_data)
      
      # Combine survival data with model data
      data <- data.frame(Time=pred_surv, Probabilities=probs, Distribution = dist[i])
      
      all_data <- rbind(all_data, data)
  }
      
      return(all_data)
 }
```

```{r, include=FALSE}
survival_curve<-survival_curves(models, distributions)
survival_curve
```
## Parametric Models
For this part of the assignment it was required to plot the survival curves of all distributions and make decision. From the plot we can see that best survival curve is the lognormal curve.

```{r, out.height='55%', echo = FALSE}
colors <- brewer.pal(n = 10, name = "Paired")
plt <- ggplot() +
    geom_line(data = survival_curve, aes(x = Time, y = Probabilities, color = Distribution), size = 1)+
    theme_minimal() +
    xlab("Time") +
    ylab("Survival Probability") +
    ggtitle("Survival Curves for Different Distributions") +
    theme(legend.position="bottom") +
    geom_abline(intercept = 0, slope = 0, linetype = "dashed", color = "gray")
  
plt
```


**Figure 1**

To make better choice of the model we can use other statistical measures such as AIC and BIC. 
Statistically best performing models are the models with lowest AIC and BIC values. 
As we can see from the results minimum AIC(2951.151) and BIC(3039.491) is generated by the model with lognormal distribution. 
Our final choice is the model with lognormal distribution.


```{r, echo = FALSE}
decision_data <- data.frame()
for(i in seq_along(models)){
  loglikelihood<- models[[i]]$loglik
  aic<- AIC(models[[i]])
  bic<- BIC(models[[i]])
  data <- data.frame(Loglikelihood=loglikelihood, AIC=aic,BIC = bic, Distribution = distributions[i])
  decision_data <- rbind(decision_data, data)
}
min(decision_data$BIC)
min(decision_data$AIC)
decision_data#[1:2,]
```

\newpage  
Now lets find out which feutures are useful for the model. 
For the first model lets include all possible feutures and examine their significance. For significance level $\alpha = 0.1$ was choosen.
```{r, echo=FALSE}
feauture_testing_model <- survreg(surv_obj ~ age + marital + address + income + ed + retire + gender + voice + internet + forward + custcat, data = telco, dist="lognormal")
s = summary(feauture_testing_model)
s
s$table[,4]<0.10
```
As we can see from the results P-values of some features are bigger than 0.1. Those features are forward, gender, income and retirement. To make the best model with good decisions without using non-useful features I eliminated mentioned features from the model. The regression summary of the final model is following.

```{r , echo=FALSE}
final_model<-survreg(surv_obj ~ age + marital + address  + ed  + voice + internet + custcat, data = telco, dist="lognormal")
summary(final_model)
```
```{r}
exp(coef(final_model))
```
For the interpretation of the coefficients we should look at the exponents of the coefficients which shwo the hazard ratio for each predictor.
Coefficient of age is positive and HR is 1.0374031 which indicates that for each additional year of life of customer there is a 3% increase of hazard.
HR of maritalUnmarried is 0.6369217 which indicates that Unmarried people have approximately 36 % lower hazard compared to Married.
Education levle Hazard is compared to the College Degree, taregt group.
HR of did not complete high school is 1.3815083 which means that mentioned group have 38 % higher hazard compare to target group.
HR of did high school is 1.3277135 which means that mentioned group have 32 % higher hazard compare to target group.
HR of did post-Undergrad degree is 0.9929849 which means that mentioned group have approximately 1 % lower hazard compare to target group.
HR of did some college is 1.2977840  which means that mentioned group have 29 % higher hazard compare to target group.
HR of Voice yes is 0.6497821 which menas that mentioned group has approximately 35% lower hazard compared to Voice No group.
HR of Internet yes is 0.4631241 which menas that mentioned group has approximately 55% lower hazard compared to internet No group.
Customer category is comared to the Basic service, target group. 
HR of E-service is 2.8972934 which means that mentioned group have 189 % higher hazard compare to target group.
HR of Plus Service is 2.2311654 which means that mentioned group have 123 % higher hazard compare to target group.
HR of Total Service is 2.8832641 which means that mentioned group have 188 % higher hazard compare to target group.

```{r, include=FALSE}
new_data <- data.frame(
  age = mean(telco$age), 
  marital = as.factor(names(which.max(table(telco$marital)))),
  address = mean(telco$address), 
  income = mean(telco$income), 
  ed = as.factor(names(which.max(table(telco$ed)))),
  retire = as.factor(names(which.max(table(telco$retire)))),
  gender = as.factor(names(which.max(table(telco$gender)))),
  voice = as.factor(names(which.max(table(telco$voice)))),
  internet = as.factor(names(which.max(table(telco$internet)))),
  forward = as.factor(names(which.max(table(telco$forward)))),
  custcat = as.factor(names(which.max(table(telco$custcat)))),
  tenure = median(telco$tenure) # Median tenure value for prediction
)
```


\newpage
## CLV
Based on the best model I made predictions and calculated CLV. 
For Calculating CLV I used formula 
$$CLV = MM \sum_{i=1}^{t} \frac{p_{i}}{(1+r/12)^{i-1}}$$
Assumption for monthly margin is 1300 AMD and asssumption for iscount rate(r) is 10 percent (retreived from the slides).
```{r, include=FALSE}
predictions <- predict(final_model, type="response", newdata = telco)
str(predictions)
```

```{r, include=FALSE}
predictions_data <- data.frame((predictions))
predictions_data
```


```{r , include=FALSE}
sequence = seq(1,length(colnames(predictions_data)),1)
MM = 1300 #assumption on monthly margin taken from slides
r = 0.1 # assumption on discount rate taken from the slides
for (num in sequence) {
predictions_data[,num]=predictions_data[,num]/(1+r/12)^(sequence[num]-1)
}
predictions_data
```


```{r, echo=FALSE}
predictions_data$CLV=MM*rowSums(predictions_data)
summary(predictions_data$CLV)
head(predictions_data)
```

```{r, echo=FALSE}
examine_data<-head(predictions_data, 24)

ggplot(examine_data,aes(x=CLV))+labs(title = "CLV Distribution")+
geom_histogram() #+ scale_x_continuous(labels = scales::label_number_si())
```
\newpage 
Now lets look at CLVs and compare them by various features. For simplicity of interpretation I took only first 24 months.
From the Figure two we can see difference of CLV among Males and Females. We can see that Males are less likely to make big purchases in early stages of their life as a customer compared to female, but later on males are making more consistent and highrer value purchases compared to the females. Female CLV have spikes while Male CLV doesn't have significant spikes. For both Males and females we can see that they make one single big purchase at start and consistent small purchases later.
```{r, echo=FALSE}
options(scipen = 999)
telco$CLV = predictions_data$CLV
examine_data_telco<-head(telco, 24)
ggplot(examine_data_telco,aes(x=CLV, color=gender))+
labs(title = "CLV Density By Gender")+
geom_density()
```
**Figure: 2**

\newpage
On the figure three I am comparing CLV's of Marries and Unmarried people. We can see that Single people tend to make Big purchases at the start of their journey as a customer, but later on that disengage and do not make consistent purchases of high value. On the other side married people after initial big purchase, are making consistent little purchases later. The Spike on the end of the graph for unmarried people can be explained by them, not using serivices for long time and later on reengaging with again.
```{r, echo=FALSE}
options(scipen = 999)
ggplot(examine_data_telco,aes(x=CLV, color=marital))+
labs(title = "CLV Density By Marital Status")+
geom_density()
```
**Figure: 3**

\newpage
For the third criterion of comparison I took education level of the customers. We can see the results on the fourth figure. As we can see most consistent customers are the ones that did not complete high school, since they are making persistent purchases over time. The customers with Post-Undergraduate digrees are the most likely to make high value purchases in the first stages and later on do not make any big purchases. This can be explained by high incomes of this type of customers, that choose the best products right on the start. The curve of people who did not complete high school indicates that they make non consistent purchase overtime, this can indicate that they are most likely to change plans and services, experimenting with various products. Customers with high school degrees have similar behaviour to post undergraduate degrees, with the difference tht they make lower price purchases at the start, but overall they are consistent.
```{r, echo=FALSE}
options(scipen = 999)
telco$CLV = predictions_data$CLV
examine_data_telco<-head(telco, 24)
ggplot(examine_data_telco,aes(x=CLV, color=ed))+
labs(title = "CLV Density By Education Level")+
geom_density()
```
**Figure: 4**
Based on all of the findings I think that most valuable clients are Married people for the long run. They are most likely to have consistent purchases over time, and consistency of purchases is a good indicator for the business. Next Valuable group is male customers. They are also showing consistent purchase pattern. From the point of view of education, it seems like customer who didn't complete high school are more likely to have a lot of purchases. Customers with Post-Undergraduate degrees will also be valuable for the business, since they are making high value purchases, which will surely benefit the business. Overall I think the most valuable clients considering all of the facts combined(consistency, high value purchases) are Married Males. 

\newpage
## Retention
For calculating retention rate of the customer on the first year first I calculated churn_rate and multiplied churn rate with overall numbers of customers(assuming that data includes all customers). To get the number of at risk customers I multiplied number of customers by churn rate. Then we get the retention budget by multiplying number of at risk customer with our average CLV. As a result I got that retention budget will be 3.937.142 drams for one year.

```{r}
churn_rate <- mean(predictions <= 12) #12 because for yeraly estimation we need 12 months

total_subscribers <- nrow(telco)

at_risk_subscribers <- total_subscribers * churn_rate

average_clv <- mean(telco$CLV)  

retention_budget <- at_risk_subscribers * average_clv
retention_budget
```

Suggestions for retention. 
To decrease the retention rate it is important to segment at risk customers. After the segmentation we should find out whether those customers bring high value to the company. If the customers are not bringing high value to the company it is not reasonable to spend budget on retaining those customers. For the at risk customers who bring high value to the company, we need to construct specific offers in order to retain them. Specific orders can include specialized plans for narrow group of customers, for example Unlimited internet for customers with high internet usage. For customer retention we can offer specific offers and discounts to some groups. 
One more good strategy for customer consistent retention being in contact with the customers, throughout their life in the company. For example, making surveys od satisfication from time to time, or organizing events for customers to increase customer loyalty.

